{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the 20 newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian', \n",
    "              'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "    categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(twenty_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\nSubject: help: Splitting a trimming region along a mesh \\nOrganization: University Of Kentucky, Dept. of Math Sciences\\nLines: 28\\n\\n\\n\\n\\tHi,\\n\\n\\tI have a problem, I hope some of the 'gurus' can help me solve.\\n\\n\\tBackground of the problem:\\n\\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \\n\\tmapping of a 3d Bezier patch into 2d. The area in this domain\\n\\twhich is inside a trimming loop had to be rendered. The trimming\\n\\tloop is a set of 2d Bezier curve segments.\\n\\tFor the sake of notation: the mesh is made up of cells.\\n\\n\\tMy problem is this :\\n\\tThe trimming area has to be split up into individual smaller\\n\\tcells bounded by the trimming curve segments. If a cell\\n\\tis wholly inside the area...then it is output as a whole ,\\n\\telse it is trivially rejected. \\n\\n\\tDoes any body know how thiss can be done, or is there any algo. \\n\\tsomewhere for doing this.\\n\\n\\tAny help would be appreciated.\\n\\n\\tThanks, \\n\\tAni.\\n-- \\nTo get irritated is human, to stay cool, divine.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sun\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.graphics\\\\38479'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.filenames[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\n",
      "Subject: help: Splitting a trimming region along a mesh \n",
      "Organization: University Of Kentucky, Dept. of Math Sciences\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(twenty_train.data[1].split(\"\\n\")[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target_names[twenty_train.target[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35482)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(min_df=1, stop_words='english')\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4683"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency times Inverse Document Frequency\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedCountVectorzier(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyser = super(StemmedCountVectorzier, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyser(doc))\n",
    "vectorizer = StemmedCountVectorzier(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StemmedCountVectorzier(analyzer='word', binary=False, decode_error='strict',\n",
       "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "            ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "            strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "            tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stemmed_counts = vectorizer.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 26888)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_stemmed_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "def tfidf(term, doc, docset):\n",
    "    tf = float(doc.count(term)) / sum(doc.count(term) for doc in docset)\n",
    "    idf = np.log(float(len(docset)) / (len([doc for doc in docset if term in doc])))\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "tfidf_vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 预处理阶段将原始帖子变成小写字母形式（父类中完成）\n",
    "- 在词语切分阶段提取所有单词（父类中完成）\n",
    "- 将每个词语转换成词干形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StemmedTfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "            ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "            smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "            sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "            tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 26888)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_stemmed_tfidf = tfidf_vectorizer.fit_transform(twenty_train.data)\n",
    "X_train_stemmed_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**文本预处理**：\n",
    "\n",
    "- 切分文本\n",
    "- 扔掉出现过分频繁，对测试集预测无帮助的词语\n",
    "- 扔掉出现出现频率很低，只有很小可能出现在测试集的词语\n",
    "- 统计剩余词语\n",
    "- 考虑整个语料集合，从词频统计中计算TF-IDF值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词袋模型**：简单有效，局限性：\n",
    "\n",
    "- 不能涵盖词语间的关联关系\n",
    "- 不能正确捕捉否定关系\n",
    "- 对于拼写错误的词语会处理失败"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.pipeline.Pipeline"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "          ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "          strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "          tokenizer=None, vocabulary=None)),\n",
       " ('tfidf',\n",
       "  TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       " ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8348868175765646"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "        categories=categories, shuffle=True, random_state=42)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.97      0.60      0.74       319\n",
      "         comp.graphics       0.96      0.89      0.92       389\n",
      "               sci.med       0.97      0.81      0.88       396\n",
      "soc.religion.christian       0.65      0.99      0.78       398\n",
      "\n",
      "           avg / total       0.88      0.83      0.84      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "                target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[192,   2,   6, 119],\n",
       "       [  2, 347,   4,  36],\n",
       "       [  2,  11, 322,  61],\n",
       "       [  2,   2,   1, 393]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1,1), (1,2)],\n",
    "             'tfidf__use_idf': (True, False),\n",
    "             'clf__alpha': (0.01, 0.1, 1, 3, 10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'vect', 'tfidf', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'clf__alpha', 'clf__class_prior', 'clf__fit_prior'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soc.religion.christian'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.01\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sun\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\sun\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\sun\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\sun\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\sun\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.08843104, 0.27639254, 0.08054733, 0.29578638, 0.0934155 ,\n",
       "        0.30748407, 0.06834888, 0.30781865, 0.07185777, 0.33372124,\n",
       "        0.07085522, 0.31082638, 0.08104865, 0.30313889, 0.08689825,\n",
       "        0.31116025, 0.06985323, 0.31884797, 0.07436474, 0.27439658]),\n",
       " 'mean_score_time': array([0.03313661, 0.060661  , 0.03676494, 0.05865645, 0.03592865,\n",
       "        0.06951857, 0.02790713, 0.05631638, 0.02606956, 0.07002012,\n",
       "        0.02640327, 0.06066179, 0.03442494, 0.05765367, 0.03993956,\n",
       "        0.07018685, 0.02590156, 0.05798761, 0.02857598, 0.05949624]),\n",
       " 'mean_test_score': array([0.92  , 0.93  , 0.9175, 0.91  , 0.91  , 0.8775, 0.8375, 0.81  ,\n",
       "        0.7075, 0.6925, 0.5825, 0.5775, 0.5725, 0.5725, 0.48  , 0.485 ,\n",
       "        0.4775, 0.49  , 0.435 , 0.445 ]),\n",
       " 'mean_train_score': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.97498428, 0.9924812 , 0.94248261, 0.98247491,\n",
       "        0.76000864, 0.80874471, 0.82873382, 0.89247933, 0.59750969,\n",
       "        0.62498005, 0.65000516, 0.70128035, 0.50247341, 0.51745469]),\n",
       " 'param_clf__alpha': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 1, 1, 1, 1,\n",
       "                    3, 3, 3, 3, 10, 10, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__use_idf': masked_array(data=[True, True, False, False, True, True, False, False,\n",
       "                    True, True, False, False, True, True, False, False,\n",
       "                    True, True, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'clf__alpha': 0.01,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 0.1, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 0.1, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 0.1, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 0.1, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 1, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 1, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 1, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 1, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 3, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 3, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 3, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 3, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 10, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 10, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 10, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 10, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}],\n",
       " 'rank_test_score': array([ 2,  1,  3,  4,  4,  6,  7,  8,  9, 10, 11, 12, 13, 13, 17, 16, 18,\n",
       "        15, 20, 19]),\n",
       " 'split0_test_score': array([0.91044776, 0.90298507, 0.8880597 , 0.88059701, 0.88059701,\n",
       "        0.85820896, 0.81343284, 0.73880597, 0.61940299, 0.58208955,\n",
       "        0.51492537, 0.5       , 0.50746269, 0.5       , 0.42537313,\n",
       "        0.39552239, 0.41791045, 0.43283582, 0.38059701, 0.3358209 ]),\n",
       " 'split0_train_score': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.96240602, 0.97744361, 0.92857143, 0.96240602,\n",
       "        0.76691729, 0.80451128, 0.81578947, 0.87593985, 0.60526316,\n",
       "        0.60902256, 0.65413534, 0.72556391, 0.48120301, 0.48120301]),\n",
       " 'split1_test_score': array([0.90225564, 0.91729323, 0.90977444, 0.89473684, 0.91729323,\n",
       "        0.85714286, 0.83458647, 0.82706767, 0.72180451, 0.71428571,\n",
       "        0.61654135, 0.60902256, 0.60150376, 0.59398496, 0.4962406 ,\n",
       "        0.53383459, 0.5037594 , 0.4962406 , 0.44360902, 0.5112782 ]),\n",
       " 'split1_train_score': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.98127341, 1.        , 0.94382022, 0.99250936,\n",
       "        0.75655431, 0.8164794 , 0.84644195, 0.91011236, 0.58801498,\n",
       "        0.62546816, 0.64794007, 0.68913858, 0.51310861, 0.54681648]),\n",
       " 'split2_test_score': array([0.94736842, 0.96992481, 0.95488722, 0.95488722, 0.93233083,\n",
       "        0.91729323, 0.86466165, 0.86466165, 0.78195489, 0.78195489,\n",
       "        0.61654135, 0.62406015, 0.60902256, 0.62406015, 0.51879699,\n",
       "        0.52631579, 0.5112782 , 0.54135338, 0.48120301, 0.4887218 ]),\n",
       " 'split2_train_score': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.98127341, 1.        , 0.95505618, 0.99250936,\n",
       "        0.75655431, 0.80524345, 0.82397004, 0.89138577, 0.59925094,\n",
       "        0.64044944, 0.64794007, 0.68913858, 0.51310861, 0.52434457]),\n",
       " 'std_fit_time': array([0.01796268, 0.01451801, 0.01961991, 0.0022793 , 0.0254979 ,\n",
       "        0.02669304, 0.0022542 , 0.00568635, 0.00642464, 0.01930976,\n",
       "        0.00165391, 0.00147585, 0.02103768, 0.00429941, 0.02788995,\n",
       "        0.02160691, 0.00250078, 0.00929816, 0.00103022, 0.01465236]),\n",
       " 'std_score_time': array([0.01249071, 0.00349732, 0.01480611, 0.0064335 , 0.01098667,\n",
       "        0.01377402, 0.00125012, 0.00047204, 0.00187626, 0.0119921 ,\n",
       "        0.00125023, 0.00255615, 0.01252576, 0.00356844, 0.01022789,\n",
       "        0.00967802, 0.00118247, 0.00333379, 0.00040919, 0.00863505]),\n",
       " 'std_test_score': array([0.01960387, 0.028778  , 0.02783828, 0.03220282, 0.02175114,\n",
       "        0.0280887 , 0.02102778, 0.05280447, 0.06716564, 0.08308033,\n",
       "        0.04796177, 0.05534708, 0.04626258, 0.05289859, 0.03984786,\n",
       "        0.06358159, 0.04240525, 0.04454775, 0.04154433, 0.07803485]),\n",
       " 'std_train_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00889417, 0.01063318, 0.01085365, 0.01419085,\n",
       "        0.00488516, 0.00547742, 0.01295926, 0.01397228, 0.00714837,\n",
       "        0.01283461, 0.00292048, 0.01717107, 0.01504045, 0.02722603])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 4296.482\n",
      "Iteration  1, inertia 2187.504\n",
      "Iteration  2, inertia 2180.463\n",
      "Iteration  3, inertia 2177.597\n",
      "Iteration  4, inertia 2176.348\n",
      "Iteration  5, inertia 2175.475\n",
      "Iteration  6, inertia 2174.642\n",
      "Iteration  7, inertia 2173.903\n",
      "Iteration  8, inertia 2173.165\n",
      "Iteration  9, inertia 2172.320\n",
      "Iteration 10, inertia 2171.513\n",
      "Iteration 11, inertia 2170.827\n",
      "Iteration 12, inertia 2169.797\n",
      "Iteration 13, inertia 2169.004\n",
      "Iteration 14, inertia 2168.392\n",
      "Iteration 15, inertia 2167.943\n",
      "Iteration 16, inertia 2167.757\n",
      "Iteration 17, inertia 2167.484\n",
      "Iteration 18, inertia 2167.286\n",
      "Iteration 19, inertia 2167.218\n",
      "Iteration 20, inertia 2167.145\n",
      "Iteration 21, inertia 2166.945\n",
      "Iteration 22, inertia 2165.632\n",
      "Iteration 23, inertia 2164.079\n",
      "Iteration 24, inertia 2164.006\n",
      "Iteration 25, inertia 2164.001\n",
      "Iteration 26, inertia 2163.988\n",
      "Iteration 27, inertia 2163.985\n",
      "Converged at iteration 27: center shift 0.000000e+00 within tolerance 3.637172e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "    n_clusters=4, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 4 \n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=num_clusters, init='random', \n",
    "            n_init=1, verbose=1)\n",
    "km.fit(X_train_stemmed_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3, ..., 2, 0, 2])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "> 《Building Machine Learning Systems with Python》"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
