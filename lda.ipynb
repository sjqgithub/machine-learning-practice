{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpora and Vector Spaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sun\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2018-06-03 11:41:01,275 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From Strings to Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "        for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
       " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "        for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-03 11:41:22,698 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-06-03 11:41:22,698 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "2018-06-03 11:41:22,699 : INFO : saving Dictionary object under ./data/deerwester.dict, separately None\n",
      "2018-06-03 11:41:22,704 : INFO : saved ./data/deerwester.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('./data/deerwester.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-03 11:44:00,809 : INFO : storing corpus in Matrix Market format to ./data/deerwester.mm\n",
      "2018-06-03 11:44:00,822 : INFO : saving sparse matrix to ./data/deerwester.mm\n",
      "2018-06-03 11:44:00,822 : INFO : PROGRESS: saving document #0\n",
      "2018-06-03 11:44:00,823 : INFO : saved 9x12 matrix, density=25.926% (28/108)\n",
      "2018-06-03 11:44:00,827 : INFO : saving MmCorpus index to ./data/deerwester.mm.index\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('./data/deerwester.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For people familiar with scikit learn, doc2bow() has similar behaviors as calling transform() on CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "pprint(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus Streaming â€“ One Document at a Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "    def __iter__(self):\n",
    "        for line in open(self.f):\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyCorpus object at 0x000002284E768940>\n"
     ]
    }
   ],
   "source": [
    "corpus_memory_friendly = MyCorpus('./data/mycorpus.txt')\n",
    "print(corpus_memory_friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "for vector in corpus_memory_friendly:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to process the text. Similarly, to construct the dictionary without loading all texts into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-03 12:04:09,734 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-06-03 12:04:09,735 : INFO : built Dictionary(42 unique tokens: ['abc', 'applications', 'computer', 'for', 'human']...) from 9 documents (total 69 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "from six import iteritems\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('./data/mycorpus.txt'))\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist\n",
    "           if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs)\n",
    "           if docfreq == 1]\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "dictionary.compactify()\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics and Transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-03 15:30:08,374 : INFO : loading Dictionary object from ./data/deerwester.dict\n",
      "2018-06-03 15:30:08,375 : INFO : loaded ./data/deerwester.dict\n",
      "2018-06-03 15:30:08,376 : INFO : loaded corpus index from ./data/deerwester.mm.index\n",
      "2018-06-03 15:30:08,376 : INFO : initializing cython corpus reader from ./data/deerwester.mm\n",
      "2018-06-03 15:30:08,377 : INFO : accepted corpus with 9 documents, 12 features, 28 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used files generated from first tutorial\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if(os.path.exists('./data/deerwester.dict')):\n",
    "    dictionary = corpora.Dictionary.load('./data/deerwester.dict')\n",
    "    corpus = corpora.MmCorpus('./data/deerwester.mm')\n",
    "    print(\"used files generated from first tutorial\")\n",
    "else:\n",
    "    print(\"Please run first tutorial to generate the set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-03 15:30:09,483 : INFO : collecting document frequencies\n",
      "2018-06-03 15:30:09,486 : INFO : PROGRESS: processing document #0\n",
      "2018-06-03 15:30:09,488 : INFO : calculating IDF weights for 9 documents and 11 features (28 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-03 15:30:25,187 : INFO : using serial LSI version on this node\n",
      "2018-06-03 15:30:25,187 : INFO : updating model with new documents\n",
      "2018-06-03 15:30:25,189 : INFO : preparing a new chunk of documents\n",
      "2018-06-03 15:30:25,189 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-06-03 15:30:25,190 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2018-06-03 15:30:25,190 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2018-06-03 15:30:25,213 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2018-06-03 15:30:25,228 : INFO : computing the final decomposition\n",
      "2018-06-03 15:30:25,229 : INFO : keeping 2 factors (discarding 47.565% of energy spectrum)\n",
      "2018-06-03 15:30:25,232 : INFO : processed documents up to #9\n",
      "2018-06-03 15:30:25,233 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2018-06-03 15:30:25,234 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\n",
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-03 15:30:44,372 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2018-06-03 15:30:44,372 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  '-0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0660078339609046), (1, -0.5200703306361845)]\n",
      "[(0, 0.19667592859142663), (1, -0.7609563167700046)]\n",
      "[(0, 0.08992639972446548), (1, -0.7241860626752509)]\n",
      "[(0, 0.07585847652178265), (1, -0.6320551586003429)]\n",
      "[(0, 0.1015029918498025), (1, -0.5737308483002957)]\n",
      "[(0, 0.7032108939378305), (1, 0.16115180214025926)]\n",
      "[(0, 0.8774787673119824), (1, 0.16758906864659573)]\n",
      "[(0, 0.9098624686818569), (1, 0.14086553628719178)]\n",
      "[(0, 0.6165825350569278), (1, -0.05392907566389278)]\n"
     ]
    }
   ],
   "source": [
    "# both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "for doc in corpus_lsi:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-03 15:32:16,067 : INFO : saving Projection object under ./data/model.lsi.projection, separately None\n",
      "2018-06-03 15:32:16,072 : INFO : saved ./data/model.lsi.projection\n",
      "2018-06-03 15:32:16,072 : INFO : saving LsiModel object under ./data/model.lsi, separately None\n",
      "2018-06-03 15:32:16,073 : INFO : not storing attribute projection\n",
      "2018-06-03 15:32:16,073 : INFO : not storing attribute dispatcher\n",
      "2018-06-03 15:32:16,078 : INFO : saved ./data/model.lsi\n",
      "2018-06-03 15:32:16,079 : INFO : loading LsiModel object from ./data/model.lsi\n",
      "2018-06-03 15:32:16,079 : INFO : loading id2word recursively from ./data/model.lsi.id2word.* with mmap=None\n",
      "2018-06-03 15:32:16,080 : INFO : setting ignored attribute projection to None\n",
      "2018-06-03 15:32:16,081 : INFO : setting ignored attribute dispatcher to None\n",
      "2018-06-03 15:32:16,081 : INFO : loaded ./data/model.lsi\n",
      "2018-06-03 15:32:16,082 : INFO : loading LsiModel object from ./data/model.lsi.projection\n",
      "2018-06-03 15:32:16,083 : INFO : loaded ./data/model.lsi.projection\n"
     ]
    }
   ],
   "source": [
    "lsi.save('./data/model.lsi')\n",
    "lsi = models.LsiModel.load('./data/model.lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-03 16:10:22,124 : INFO : using symmetric alpha at 0.01\n",
      "2018-06-03 16:10:22,125 : INFO : using symmetric eta at 0.01\n",
      "2018-06-03 16:10:22,126 : INFO : using serial LDA version on this node\n",
      "2018-06-03 16:10:22,127 : INFO : running online (single-pass) LDA training, 100 topics, 1 passes over the supplied corpus of 9 documents, updating model once every 9 documents, evaluating perplexity every 9 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2018-06-03 16:10:22,127 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2018-06-03 16:10:22,136 : INFO : -124.785 per-word bound, 36644080589458173557036207491403218944.0 perplexity estimate based on a held-out corpus of 9 documents with 29 words\n",
      "2018-06-03 16:10:22,136 : INFO : PROGRESS: pass 0, at document #9/9\n",
      "2018-06-03 16:10:22,140 : INFO : topic #8 (0.010): 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "2018-06-03 16:10:22,140 : INFO : topic #60 (0.010): 0.324*\"computer\" + 0.324*\"interface\" + 0.324*\"human\" + 0.003*\"eps\" + 0.003*\"graph\" + 0.003*\"trees\" + 0.003*\"system\" + 0.003*\"user\" + 0.003*\"survey\" + 0.003*\"time\"\n",
      "2018-06-03 16:10:22,141 : INFO : topic #34 (0.010): 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "2018-06-03 16:10:22,141 : INFO : topic #94 (0.010): 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "2018-06-03 16:10:22,142 : INFO : topic #78 (0.010): 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "2018-06-03 16:10:22,142 : INFO : topic diff=87.158096, rho=1.000000\n"
     ]
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lda = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(60, 0.7525)]\n",
      "[(31, 0.85857147)]\n",
      "[(35, 0.80200005)]\n",
      "[(73, 0.80200005)]\n",
      "[(17, 0.7525)]\n",
      "[(12, 0.505)]\n",
      "[(67, 0.67)]\n",
      "[(47, 0.7525)]\n",
      "[(81, 0.7525)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_lda:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-03 16:43:36,389 : INFO : (0, '0.253*time + 0.169*minors + 0.115*user + 0.115*human + 0.100*survey + 0.099*system + 0.042*response + 0.032*trees + 0.030*eps + 0.022*graph')\n",
      "2018-06-03 16:43:36,390 : INFO : (1, '0.329*survey + 0.178*time + 0.100*user + 0.085*eps + 0.058*human + 0.055*trees + 0.043*response + 0.042*interface + 0.038*computer + 0.037*graph')\n",
      "2018-06-03 16:43:36,390 : INFO : (2, '0.168*response + 0.168*minors + 0.143*trees + 0.143*survey + 0.141*human + 0.057*time + 0.043*system + 0.041*graph + 0.034*computer + 0.029*interface')\n",
      "2018-06-03 16:43:36,390 : INFO : (3, '0.328*trees + 0.200*survey + 0.127*eps + 0.112*human + 0.071*time + 0.046*graph + 0.033*minors + 0.029*computer + 0.025*system + 0.020*interface')\n",
      "2018-06-03 16:43:36,391 : INFO : (4, '0.254*survey + 0.172*human + 0.170*interface + 0.126*user + 0.102*trees + 0.055*time + 0.035*minors + 0.030*graph + 0.022*response + 0.019*computer')\n",
      "2018-06-03 16:43:36,391 : INFO : (5, '0.329*human + 0.138*interface + 0.125*minors + 0.109*graph + 0.107*system + 0.067*time + 0.058*trees + 0.022*computer + 0.017*user + 0.011*response')\n",
      "2018-06-03 16:43:36,392 : INFO : (6, '0.207*interface + 0.192*response + 0.128*time + 0.101*eps + 0.099*survey + 0.086*graph + 0.075*minors + 0.045*system + 0.041*user + 0.014*trees')\n",
      "2018-06-03 16:43:36,393 : INFO : (7, '0.265*survey + 0.133*response + 0.133*computer + 0.106*eps + 0.072*trees + 0.069*minors + 0.053*human + 0.050*graph + 0.045*time + 0.033*interface')\n",
      "2018-06-03 16:43:36,393 : INFO : (8, '0.215*human + 0.207*survey + 0.166*minors + 0.086*response + 0.075*time + 0.063*interface + 0.050*graph + 0.047*trees + 0.036*system + 0.026*computer')\n",
      "2018-06-03 16:43:36,393 : INFO : (9, '0.299*trees + 0.201*minors + 0.136*interface + 0.102*survey + 0.077*time + 0.051*human + 0.046*eps + 0.040*graph + 0.028*user + 0.014*system')\n",
      "2018-06-03 16:43:36,394 : INFO : (10, '0.277*minors + 0.235*response + 0.107*human + 0.083*graph + 0.056*time + 0.047*survey + 0.046*eps + 0.045*system + 0.040*interface + 0.036*computer')\n",
      "2018-06-03 16:43:36,394 : INFO : (11, '0.266*interface + 0.116*time + 0.107*computer + 0.090*minors + 0.080*human + 0.074*trees + 0.070*graph + 0.067*system + 0.048*survey + 0.047*user')\n",
      "2018-06-03 16:43:36,395 : INFO : (12, '0.225*system + 0.173*minors + 0.146*survey + 0.110*time + 0.108*human + 0.056*eps + 0.053*interface + 0.041*graph + 0.039*computer + 0.031*user')\n",
      "2018-06-03 16:43:36,395 : INFO : (13, '0.235*survey + 0.136*human + 0.106*computer + 0.105*user + 0.087*graph + 0.080*response + 0.075*minors + 0.066*system + 0.042*eps + 0.030*time')\n",
      "2018-06-03 16:43:36,395 : INFO : (14, '0.389*time + 0.195*system + 0.191*response + 0.051*trees + 0.048*human + 0.041*survey + 0.033*minors + 0.025*eps + 0.019*computer + 0.006*interface')\n",
      "2018-06-03 16:43:36,395 : INFO : (15, '0.274*minors + 0.136*eps + 0.126*time + 0.102*human + 0.084*interface + 0.062*system + 0.054*response + 0.047*user + 0.046*trees + 0.031*graph')\n",
      "2018-06-03 16:43:36,395 : INFO : (16, '0.243*system + 0.165*time + 0.106*response + 0.093*trees + 0.068*minors + 0.067*human + 0.060*graph + 0.046*user + 0.044*eps + 0.043*computer')\n",
      "2018-06-03 16:43:36,396 : INFO : (17, '0.358*user + 0.156*eps + 0.155*time + 0.122*trees + 0.062*graph + 0.060*computer + 0.030*minors + 0.016*response + 0.015*interface + 0.013*human')\n",
      "2018-06-03 16:43:36,397 : INFO : (18, '0.231*user + 0.178*eps + 0.126*computer + 0.113*survey + 0.108*response + 0.072*interface + 0.041*graph + 0.035*trees + 0.033*minors + 0.024*human')\n",
      "2018-06-03 16:43:36,397 : INFO : (19, '0.211*computer + 0.208*interface + 0.159*response + 0.140*human + 0.086*trees + 0.053*graph + 0.052*survey + 0.025*eps + 0.024*system + 0.024*time')\n"
     ]
    }
   ],
   "source": [
    "hdp = models.HdpModel(corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_hdp = hdp[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.06741614680567234), (1, 0.7920927532224492), (2, 0.035688734653243494), (3, 0.026529884585874488), (4, 0.019860377677366974), (5, 0.014872695960563825), (6, 0.011150500194695462)]\n",
      "[(0, 0.4062036335156195), (1, 0.5135419015170811), (2, 0.020373199057003757), (3, 0.015154786977715686), (4, 0.011348147168246982)]\n",
      "[(0, 0.05760860426667446), (1, 0.8301022749912499), (2, 0.02845171682189931), (3, 0.021219954643810615), (4, 0.01588777607521441), (5, 0.0118981475183838)]\n",
      "[(0, 0.8491454873551538), (1, 0.03856316322806118), (2, 0.028446905373780192), (3, 0.021227456329563976), (4, 0.01588731851671879), (5, 0.0118981435379153)]\n",
      "[(0, 0.8114113730097752), (1, 0.0480967088486063), (2, 0.03570462828068052), (3, 0.026515982512872547), (4, 0.019859229646241316), (5, 0.01487267060595147), (6, 0.011150500195747818)]\n",
      "[(0, 0.1278178000073639), (1, 0.09535577664685667), (2, 0.5671660453117332), (3, 0.05311736296509713), (4, 0.03971885447243531), (5, 0.02974534650477296), (6, 0.02230100029150766), (7, 0.01670528523805235), (8, 0.012442169149124783)]\n",
      "[(0, 0.08763233559812991), (1, 0.7248387992376808), (2, 0.04773826811228807), (3, 0.03542818714502497), (4, 0.026479618059956665), (5, 0.019830249068489545), (6, 0.014867333578258151), (7, 0.01113685682538468)]\n",
      "[(0, 0.06560698595171327), (1, 0.047624961726119096), (2, 0.781964365049898), (3, 0.026532391148836626), (4, 0.0198592092027768), (5, 0.014872679851112437), (6, 0.011150500169425981)]\n",
      "[(0, 0.5356465130596558), (1, 0.3238586520100682), (2, 0.03569237916038331), (3, 0.026530875315293398), (4, 0.01985948860738793), (5, 0.01487268475665978), (6, 0.011150500190422144)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_hdp:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.07910475117444982), (1, -0.5732835243079397)]\n"
     ]
    }
   ],
   "source": [
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-03 17:50:31,629 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2018-06-03 17:50:31,630 : INFO : creating matrix with 9 documents and 2 features\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus]) # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.9999408), (1, 0.9946708), (2, 0.9999428), (3, 0.999879), (4, 0.99935204), (5, -0.08804217), (6, -0.0515742), (7, -0.023664713), (8, 0.1938726)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.9999428),\n",
      " (0, 0.9999408),\n",
      " (3, 0.999879),\n",
      " (4, 0.99935204),\n",
      " (1, 0.9946708),\n",
      " (8, 0.1938726),\n",
      " (7, -0.023664713),\n",
      " (6, -0.0515742),\n",
      " (5, -0.08804217)]\n"
     ]
    }
   ],
   "source": [
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "pprint(sims) # print sorted (document number, similarity score) 2-tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://radimrehurek.com/gensim/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
